#!/usr/bin/env python3
import boto3
import os
import sys
import argparse
import datetime

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parse command-line arguments
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
parser = argparse.ArgumentParser(description="Upload compliance reports to S3 and trigger Glue crawler.")
parser.add_argument("--bucket", required=True, help="S3 bucket name")
parser.add_argument("--region", required=True, help="AWS region")
parser.add_argument("--os", required=True, help="Operating system (ubuntu/rhel/amazonlinux)")
parser.add_argument("--build_date", required=True, help="Build date (YYYYMMDD)")
args = parser.parse_args()

BUCKET = args.bucket
REGION = args.region
OS_NAME = args.os
BUILD_DATE = args.build_date
REPORTS_PATH = "./reports"
CRAWLER = "cloud-secure-infra-dev-compliance-crawler"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Initialize clients
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
s3 = boto3.client("s3", region_name=REGION)
glue = boto3.client("glue", region_name=REGION)

print(f"ğŸ§¾ Using bucket: {BUCKET}")
print(f"ğŸ§© OS: {OS_NAME}")
print(f"ğŸ“… Build date: {BUILD_DATE}")
print(f"ğŸ“‚ Local path: {REPORTS_PATH}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Upload all report files (JSON/TXT/XML)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uploaded = 0
timestamp = datetime.datetime.utcnow().strftime("%Y%m%d%H%M%S")

for root, _, files in os.walk(REPORTS_PATH):
    for f in files:
        if f.endswith((".json", ".txt", ".xml")):
            file_path = os.path.join(root, f)
            # Structured S3 key: <os>/<date>/<filename>
            key = f"{OS_NAME}/{BUILD_DATE}/{f}"
            print(f"ğŸ“¤ Uploading {file_path} â†’ s3://{BUCKET}/{key}")
            s3.upload_file(file_path, BUCKET, key)
            uploaded += 1

if uploaded == 0:
    print("âš ï¸ No report files found in ./reports â€” nothing to upload.")
else:
    print(f"âœ… Uploaded {uploaded} report file(s) to s3://{BUCKET}/{OS_NAME}/{BUILD_DATE}/")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Trigger Glue Crawler
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    print(f"ğŸš€ Triggering Glue Crawler: {CRAWLER}")
    glue.start_crawler(Name=CRAWLER)
    print("âœ… Glue crawler triggered successfully.")
except Exception as e:
    print(f"âŒ Failed to trigger Glue Crawler: {e}")
    sys.exit(1)
